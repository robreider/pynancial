{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3ca6e4",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px\">Chapter 12 </h1>\n",
    "\n",
    "<h1 style=\"font-size: 30px\">Financial Planning Using Reinforcement Learning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ea760",
   "metadata": {},
   "source": [
    "When we looked at asset allocation in earlier chapters, we were essentially performing a single-period optimization. However, most financial planning decisions involve making decisions over multiple periods. And the decisions made today - not only how to allocate assets but how much to spend, whether to claim Social Security, when to retire, what accounts to withdraw from, etc. - affect decisions in the future. These multiperiod, dynamic models, which economists sometimes call “lifecycle models”, are much more complicated to optimize.\n",
    "\n",
    "In recent years, a branch of Artificial Intelligence called “reinforcement learning” has been applied to a wide range of problems involving decision-making over multiple time periods. Google famously used reinforcement learning to train a computer model to beat the best human at the game Go. Similarly, Libratus used reinforcement learning to beat some of the best poker players. And the successes are not limited to playing games. However, there is currently very little written about using reinforcement learning to solve financial planning problems.\n",
    "In this chapter, we will explain how reinforcement learning can be used to solve multiperiod wealth management problems. As usual, we will start with a simple example and build up to more realistic, complicated examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9ce00",
   "metadata": {},
   "source": [
    "## 12.1 A Goals-Based Investing Example\n",
    "\n",
    "In Chapter 5, we used Monte Carlo simulations to answer the question of whether an investor will run out of money in retirement. The simple binary outcome of success or failure is an example of “Goals-Based Investing”. Other examples of goals might be saving for a child's education or saving for a house.\n",
    "\n",
    "Hopefully, it will be easier to understand the theory behind reinforcement learning with a concrete example in mind. Consider an investor with \\\\$1 million in assets and a goal of reaching \\\\$2 million in 10 years. For now, assume there are no savings inflows or spending outflows over the ten years. We would like to compute the optimal asset allocation over time, and the probability of achieving the goal assuming the investor follows the optimal allocation. We assume the asset allocation can change once per year. \n",
    "\n",
    "The term “glide path” refers to how the asset allocation changes over time. Conventional wisdom says that the allocation to stocks should decline over time. Indeed, target-date mutual funds are designed to automatically rebalance stock and bond funds over time according to a predetermined glide path. But some authors have suggested an alternative view that equity exposure should increase over time. In this chapter, we will not only compute optimal glide paths, but you will be able to see what factors affect the shape and slope of the glide path. \n",
    "\n",
    "This starting example is simplistic and unrealistic on many levels. Perhaps the biggest shortcoming is that the goal is binary and does not make any distinction between achieving a final wealth of \\\\$2 million and \\\\$100 million. In reality, of course, even if one dies after the goals are achieved, they still may derive some benefit from knowing they are giving money to charities or children. We can modify the binary goal and use utility functions instead, which will be discussed later in the chapter. We will also discuss adding other choice variables besides asset allocation, like how much to spend each period and when to claim Social Security benefits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb381b",
   "metadata": {},
   "source": [
    "## 12.2 An Introduction to Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is a branch of machine learning that involves training an agent to make decisions based on trial and error. In this approach, an agent interacts with an environment and receives feedback in the form of rewards or penalties for the actions it takes. In our example, the agent is the investor and the environment is the stock and bond market. The goal of an agent is to learn a policy, which is a set of rules that dictate which actions to take in which situations to maximize the cumulative reward over time.\n",
    "\n",
    "Just like the way a baby learns to walk, an agent's behavior is shaped by the rewards it receives. Positive rewards encourage the agent to take similar actions in the future, while negative rewards discourage it from doing so. The agent uses this feedback to adjust its policy and improve its decision-making process. \n",
    "There are four common features that are used to describe most reinforcement learning problems: state, action, reward, and transition probabilities:\n",
    "\n",
    "* **State:** A state represents the relevant variables in the environment that affect an agent's actions. In our case, the state will be a tuple containing two variables, wealth, $\\small{W}$, and discrete time steps, $\\small{t=0,1,2, \\ldots , T}$. We will discretize wealth into a finite set of states as well. In Listing 12.1, we create this wealth grid and plot it in Figure 12.1. A few comments are in order. Because the dispersion of potential wealth gets wider over time, our grid has a funnel shape rather than a rectangular shape. We compute a rough maximum and minimum value of wealth over time and then create an equally spaced grid in log of wealth space, which is on the right-hand side of Figure 12.1. The same grid in wealth space is on the left-hand side of Figure 12.1. In Figure 12.1, we only used a wealth grid size of 11 points for illustrative purposes, but when training the model later in the chapter, we substantially increased the grid size.\n",
    "\n",
    "* **Action:** An action, as the name indicates, is a decision the agent makes that changes the state. In a video game, an action could be moving a joystick to the left or right. In our example, an investor will only be able to invest in a stock fund or a risk-free bond, so an action would be the percentage of wealth that is invested in the stock fund and one minus that percentage is the percentage allocated to the bond. Just like we discretized states, we will discretize actions into 5% increments, so the action space of all possible actions is $\\small{A=0\\%, 5\\%, 10\\%, \\ldots , 100\\%}$. A policy is a mapping from each state to the best action at that state.\n",
    "\n",
    "* **Rewards:** A reward is a single number that represents the benefit an agent receives after taking an action. The goal of reinforcement learning is to maximize the cumulative rewards over time. A reward can be negative as well, so if the goal is to finish a task as quickly as possible, a negative reward can be assigned for every time step when the task is not completed. In our goals-based investing example, the agent receives a reward of +1 at time $\\small{T}$ if his wealth at time $\\small{T}$ exceeds the goal, and the agent receives no reward otherwise. We will look at other financial planning problems later in the chapter where the agent receives a reward every period when spending is taken into account.\n",
    "\n",
    "* **Transition Probabilities:** The transition probabilities define the probabilities of moving from one state at time $\\small{t}$ to another state at time $\\small{t+1}$ after taking a particular action. An implicit assumption is that only the current state at time $\\small{t}$, and not the history of previous states before $\\small{t}$, is relevant for computing transition probabilities. This is referred to as the “Markov property”, which is a standard assumption in reinforcement learning problems. In a game of chess, for example, the state would be the positions of the pieces on the board, and the transition dynamics only depend on the current state of the board - the history of how the pieces arrived at that state is irrelevant. In the goals-based investing example, for a given wealth at time $\\small{t}$ (the state) and an asset allocation (the action), we can use the mathematics of a normal distribution to compute the transition probabilities of getting to each possible wealth state at time $\\small{t+1}$. The Python code for the transition probabilities is given in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2c21b",
   "metadata": {},
   "source": [
    "**Listing 12.1 Creating State Space for the Goals-Based Investing Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9dc16d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, sqrt, exp\n",
    "\n",
    "W0 = 1000000\n",
    "T = 10\n",
    "nW = 11 #A\n",
    "mu_stock = 0.08\n",
    "mu_bond = 0.045\n",
    "sig_stock = 0.20\n",
    "sig_bond = 0.0\n",
    "\n",
    "lnW = np.zeros((nW,T+1))\n",
    "W = np.zeros((nW, T+1))\n",
    "\n",
    "for t in range(T+1):\n",
    "    lnW_min = log(W0) + (mu_stock-0.5*sig_stock**2)*t - 2.5*sig_stock*sqrt(t)\n",
    "    lnW_max = log(W0) + (mu_stock-0.5*sig_stock**2)*t + 2.5*sig_stock*sqrt(t)\n",
    "    lnW[:,t] = np.linspace(lnW_min, lnW_max, nW) #B\n",
    "W = np.exp(lnW)\n",
    "\n",
    "plt.figure(figsize=(10, 6)) #C\n",
    "plt.subplot(1,2,1)    \n",
    "for t in range(T+1):\n",
    "    plt.scatter(t*np.ones(nW), W[:,t], color='r')\n",
    "plt.xlabel(\"time, $t$\")\n",
    "plt.ylabel(\"Wealth, $W$\")\n",
    "plt.subplot(1, 2, 2)\n",
    "for t in range(T+1):\n",
    "    plt.scatter(t*np.ones(nW), lnW[:,t], color='r')\n",
    "plt.xlabel(\"time, $t$\")\n",
    "plt.ylabel(\"Log of Wealth, $\\ln(W)$\");\n",
    "\n",
    "\n",
    "#A In the figure below, we used a grid size in wealth of 11, not 101\n",
    "#B Linear between min and max in log space\n",
    "#C Plot state spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd0b08",
   "metadata": {},
   "source": [
    "**Figure 12.1. State space in wealth-time and log wealth-time**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fefca",
   "metadata": {},
   "source": [
    "Now that we have described the reinforcement learning problem in terms of states, actions, rewards, and transition probabilities, the next step is to estimate the value at each state, which is referred to as the value function. The value function is the expected cumulative rewards, starting from that state. When the value function at each state is maximized, by choosing the best set of actions going forward, then it is the optimal value function. In some cases, a reward today is worth more than a reward in the future, so the value function computes the present value by discounting future rewards. In our case, we only have a reward at the end, so we don't have to deal with discounting, but later we will discuss other financial planning problems involving discounting.\n",
    "Before we discuss the algorithms used in reinforcement learning, we will digress and discuss an older algorithm to estimate the value function. Reinforcement learning will make more sense when we explain “dynamic programming”, and it will give us a chance to highlight the method's shortcomings and why reinforcement learning has become such a powerful tool.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e463f08",
   "metadata": {},
   "source": [
    "### 12.2.1 Solution Using Dynamic Programming\n",
    "\n",
    "The traditional method for solving our goals-based investment problem was to use dynamic programming (DP) with backward induction (also called backward recursion), which is a technique that has been employed by economists for over 50 years. This method will provide a foundation for understanding reinforcement learning in the next section.\n",
    "\n",
    "Before we can implement the dynamic programming solution, we need to compute the transition probabilities going from any wealth state at time $\\small{t}$ to possible wealth states at time $\\small{t+1}$. Listing 12.2 provides the code for estimating the transition probabilities. We start with two helper functions. The first one, `compute_mu_sigma()`, is a simple function that takes an asset allocation and computes the mean and standard deviation of the portfolio. The second helper function, `compute_midpoints()` computes the midpoints of adjacent wealth states. We will estimate the probability of transitioning to a particular discrete wealth state as the probability of wealth falling between the lower and upper midpoints of adjacent wealth states. For the extreme lowest-wealth state, the lower bound is zero wealth, and for the extreme highest-wealth state, the upper bound is infinity. We then compute the cumulative distribution function (CDF) at each midpoint, and the difference between adjacent CDF's is the area under a normal curve, which corresponds to the probability of landing between the midpoints. You may notice an unusual `np.float64()` wrapper when we are computing CDF. The reason is that if we are considering an allocation of 100% risk-free bonds, we would be dividing by zero volatility, which would normally produce an error, rather than +infinity or -infinity, which is what we want. Finally, it should be noted that to get the transition probabilities, Listing 12.2 assumes that stock returns are normally distributed, but we can easily modify the code to incorporate other distributions, like those with fat tails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098e9b9",
   "metadata": {},
   "source": [
    "**Listing 12.2 Computing Transition Probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c22d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def compute_mu_sigma(a):\n",
    "    mu = a/(nA-1)*mu_stock + (1-a/(nA-1))*mu_bond\n",
    "    sig = a/(nA-1)*sig_stock\n",
    "    return mu, sig\n",
    "\n",
    "def compute_midpoints():\n",
    "    W_midpoints = np.zeros((nW+1,T+1))\n",
    "    W_midpoints[0,:] = 0.000001\n",
    "    W_midpoints[nW,:] = np.inf\n",
    "    W_midpoints[1:nW,:] = (W[:nW-1,:]+W[1:nW,:])/2\n",
    "    return W_midpoints\n",
    "\n",
    "def compute_transition_probs(w, t, a):\n",
    "    mu, sig = compute_mu_sigma(a)\n",
    "    W_tplus1_dist = np.zeros(nW)\n",
    "    cdfs = norm.cdf(np.float64((np.log(W_midpoints[:,t+1]/w) - (mu-0.5*sig*sig))/sig)) #A\n",
    "    W_tplus1_dist = cdfs[1:nW+1] - cdfs[0:nW] #B\n",
    "    return W_tplus1_dist\n",
    "\n",
    "#A Standardize return by subtracting mean and dividing by standard deviation, and then get CDF\n",
    "#B Probability is the difference in adjacent CDF's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b54e56",
   "metadata": {},
   "source": [
    "In Figure 12.2, we illustrate the transition probabilities in going from the wealth state at node 54 (just under the \\\\$2 million goal) in the second-to-last period, $\\small{t=9}$, to possible wealth states at time $\\small{t=10}$, using the listing above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e8bb2",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"350\" height=\"750\" src=\"Fig12_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d5f3e",
   "metadata": {},
   "source": [
    "**Figure 12.2. Transition probabilities from a wealth state at time $\\small{T-1}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71233180",
   "metadata": {},
   "source": [
    "We are now ready to use dynamic programming to compute the optimal value function, and it's only a few lines of code, which is given in Listing 12.3. In our goals-based investing example, we know the value function at the last time period, $\\small{T}$. The terminal value is simply the reward at the end, which is +1 if we meet the goal and 0 if we don't. Then, going one step back in time to $\\small{T-1}$, we loop through every wealth state. At each wealth state, we then loop through every possible action, or in our case every possible asset allocation, to find the one with the highest expected value. The expected value at a given state and choice of asset allocation is the dot product of the transition probabilities of the future state and the value at those future states. The function $\\small{V(W,t)}$ is the optimal value function at each state, and we keep track of the optimal action at each state with the function $\\small{A(W,t)}$. Once we find the optimal value at each possible wealth at time $\\small{T-1}$, we go back one time step and keep repeating until we get to time step 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4cb44",
   "metadata": {},
   "source": [
    "**Listing 12.3 Dynamic Programming Solution for Goals-Based Investing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2167035",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 2000000\n",
    "nA = 21\n",
    "V = np.zeros((nW, T+1)) #A\n",
    "A = np.zeros((nW, T)) #B\n",
    "EV = np.zeros(nA) #C\n",
    "W_midpoints = compute_midpoints()\n",
    "\n",
    "for j in range(nW): #D\n",
    "    if W[j, T] > G: \n",
    "        V[j, T] = 1    \n",
    "        \n",
    "for t in range(T-1, -1, -1):\n",
    "    for j in range(nW):\n",
    "        for a in range(nA):\n",
    "            W_tplus1_dist = compute_transition_probs(W[j,t], t, a)\n",
    "            EV[a] = np.dot(W_tplus1_dist, V[:, t+1])\n",
    "        V[j,t] = EV.max() #E\n",
    "        A[j,t] = EV.argmax() #F\n",
    "\n",
    "#A Initialize the two-dimensional optimal value function\n",
    "#B Initialize the two-dimensional optimal action function\n",
    "#C Initialize the Expected Value as a function of possible actions\n",
    "#D Compute the value function in the last time period\n",
    "#E Save the optimal value for each state\n",
    "#F Save the optimal action for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724b4f0",
   "metadata": {},
   "source": [
    "The optimal value function at any state gives the expected cumulative rewards from that state going forward, and since the reward for “success” is +1 and failure is 0, the value function represents the probability of achieving the goal at any state. For example, at the starting state at $\\small{t=0}$ and initial wealth of \\\\$1 million, the value, $\\small{V(0,0)}$ is 0.68, which means there is a 68\\% chance of meeting the goal if the optimal policies are followed at each state.\n",
    "We can also compute the optimal glide path for this problem. Although most financial advice, and target date funds themselves, assume a fixed glide path over time, it should be clear from this example that the optimal asset allocation should not only be a function of time but also a function of wealth. Therefore, it is impossible to talk about the optimal glide path without talking about the path of wealth over time. Out of the trillions of possible paths of wealth, let's consider three simple paths, which are shown in Figure 12.3. In the “good” path, wealth rises by a steady 9\\%/year. In the “bad” path, wealth rises by a steady 6\\%/year. And in the “medium” path, wealth rises by 7.5\\%/year. This is, of course, an oversimplification, but it will illustrate how glide paths can depend on the wealth path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7bcca",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"500\" height=\"300\" src=\"Fig12_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b6d0b",
   "metadata": {},
   "source": [
    "**Figure 12.3. Three paths of wealth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59685aa6",
   "metadata": {},
   "source": [
    "In Figure 12.4, we show the optimal allocation of stocks for the three wealth paths, which is based on the function $A\\small{(W, t)}$ that we computed when we solved the dynamic programming problem. And the results should be fairly intuitive. For all three paths, we start out at time 0 with a fairly high allocation to stocks of 85\\%, which is reasonable given our difficult goal of doubling our wealth in 10 years. In the medium return path, the allocation to stocks gradually declines, and at time period $\\small{t=9}$, the wealth is large enough that the goal can be achieved with certainty by placing all of the assets in the risk-free bond and there is no reason to take any stock risk, given the reward structure of the problem. In the high return path, the allocation to stocks declines even more rapidly, and by time period $\\small{t=7}$, the goal can be achieved by investing in the bond for the remaining time. But in the bad return path, the allocation to stocks goes up as the wealth falls further and further behind in achieving the goal. By time $\\small{t=7}$, the best chance the investor has to achieve the goal is to place all assets in stocks and nothing in the bond. This “go-for-broke” risk-taking in the bad path is purely an artifact of how the rewards are specified, and a more realistic objective function can lead to different behaviors, which we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5cae8",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"500\" height=\"300\" src=\"Fig12_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca7cbc",
   "metadata": {},
   "source": [
    "**Figure 12.4. Glide paths for three paths of wealth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1390de5",
   "metadata": {},
   "source": [
    "There are two main problems with the dynamic programming method. First, notice that this method requires that we know the transition probabilities in order to compute expected values. There are many problems where the agent does not know the exact model of the environment. Usually in finance applications, we know the dynamics of the environment, but there are situations where it may be difficult to derive closed-form equations for the transition probabilities. However, the more serious shortcoming of DP is related to the curse of dimensionality. In our example, we had a relatively small number of states and possible actions, so the DP solution only took a few seconds to run. But many more realistic wealth problems involve more state variables and states and more potential actions at each state. In the DP solution, we looped through every single state and action to compute the value function. For example, we are including the possibility that wealth can go from an extremely low wealth at time $\\small{t}$ to a very high wealth at time $\\small{t+1}$, and then back to a very low wealth at time $\\small{t+2}$, which is extremely unlikely. In the reinforcement learning algorithm in the next section, we go forward in time rather than backward and we approximate the value function by simulating more likely paths rather than computing the value function exactly by looking at every possible path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3514b6",
   "metadata": {},
   "source": [
    "### 12.2.2 Solution Using Q-Learning\n",
    "\n",
    "Q-learning is one of the most popular algorithms used in reinforcement learning to find optimal policies. At each time step, an agent chooses an action and tries to learn the best policy through these actions. A series of time steps from $\\small{t=0}$ to $\\small{t=T}$ is called an “episode”, and agents learn by running through many episodes, each time updating their estimate of the value function. With dynamic programming in the last section, we estimated a value function for each state. In Q-learning, we estimate a function $\\small{Q}$, which is the value at not only a state but also an action, or a state-action pair. In the goals-based example, we computed $\\small{V(W,t)}$. In Q-learning, we will estimate $\\small{Q(W,t,a)}$, where a represents an action like an asset allocation. The function $\\small{Q}$ is initialized at the beginning, often with all zeros. But instead of starting at time $\\small{t=T}$ and going backwards, we go forward in time starting at $\\small{t=0}$ and $\\small{Q}$ is updated according to the following equation:\n",
    "\n",
    "$$\n",
    "\\underbrace{Q(S_t,A_t)\\vphantom{\\max_a}}_{\\text{updated }Q\\vphantom{\\max_a}} \\ \\  \\leftarrow \\ \\   \\underbrace{Q(S_t,A_t)\\vphantom{\\max_a}}_{\\text{current }Q}\\  + \\underbrace{\\ \\ \\ \\ \\ \\  \\alpha\\ \\ \\ \\ \\ \\  \\vphantom{\\max_a}}_{\\text{learning rate}}   \\left(\\underbrace{\\ \\ \\  R_{t}\\ \\ \\ \\vphantom{\\max_a}}_{\\text{reward}} + \\underbrace{\\ \\ \\ \\ \\ \\ \\gamma \\ \\ \\ \\ \\ \\ \\vphantom{\\max_a}}_{\\text{discount rate}}  \\underbrace{\\max_a Q(S_{t+1}, a)}_{\\text{optimal next }Q} \\ - \\ \\underbrace{Q(S_t,A_t)\\vphantom{\\max_a}}_{\\text{current }Q} \\right)\n",
    "$$\n",
    "\n",
    "To better understand this equation let us define a few of the parameters and terms involved in the Q-learning algorithm:\n",
    "\n",
    "* Optimal next $\\small{Q}$: The term $\\small{\\max_a Q(S_{t+1}, a)}$ is an estimate of the maximum $\\small{Q}$ value, over all possible actions, at the next time step, $\\small{t+1}$. It is only an estimate of the next period's value, so we are iteratively estimating this period's value based on our estimate of the next period's value.\n",
    "\n",
    "* Discount rate, $\\small{\\gamma}$: An agent may prefer immediate rewards over future rewards, and the discount rate 𝛾 represents the rate at which future rewards are discounted $\\small{(0<𝛾<1)}$. In the extremes, if $\\small{𝛾=1}$, then the agent has no preference for current rewards over future rewards, and if $\\small{𝛾=0}$, then the agent is myopic and only cares about immediate rewards. Note that this discount rate can be subjective and differs from the rate of interest. For example, a smoker craving a cigarette may place a large discount on a cigarette received a year from now, and that subjective discount rate can be quite different from the discount rate on a bond.\n",
    "\n",
    "* Temporal difference (term in brackets): The temporal difference is the difference between the discounted value of future rewards next period, and our estimate of the value this period. If next period's estimate is above our current estimate (a positive difference), we want to increase our current estimate, and vice versa.\n",
    "\n",
    "* Learning rate, $\\small{\\alpha}$: The learning rate controls how quickly the agent updates $\\small{Q}$ based on the temporal difference.\n",
    "\n",
    "We are now ready to implement the Q-learning algorithm in Python. We start with the same wealth-time state space grid that was used in the dynamic programming example. In dynamic programming, we had to compute the transition probabilities going from one wealth node at time $\\small{t}$ to all possible wealth nodes at $\\small{t+1}$. Q-learning is model-free. The agent does not need to know these probabilities, and indeed in some problems we are interested in, these probabilities may be difficult to compute. Therefore, instead of the function in Listing 12.2 that computes transition probabilities, Q-learning will use the function in Listing 12.4 that computes a random transition from wealth at time $\\small{t}$ to a new wealth at time $\\small{t+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccb6c7",
   "metadata": {},
   "source": [
    "**Listing 12.4 Generating Random Transition States** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c832cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(w, t, a):\n",
    "    mu, sig = compute_mu_sigma(a)\n",
    "    W_tplus1 = w * exp(mu-0.5*sig*sig + sig*np.random.normal())\n",
    "    W_tplus1_idx = (np.abs(W[:,t+1] - W_tplus1)).argmin() #A\n",
    "    return W_tplus1_idx\n",
    "\n",
    "#A Find the closest wealth node next period  that corresponds to the next period's wealth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489ead1",
   "metadata": {},
   "source": [
    "Listing 12.5 gives the code for updating $\\small{Q}$ according to the equation above. One fundamental feature of reinforcement learning that we have not yet addressed is the tradeoff between exploration and exploitation. During the learning process, an agent must choose an action at each state. Exploitation means the agent takes an action that is expected to give the highest reward, based on its current knowledge. In contrast, exploration means the agent takes random actions to learn more about the environment.  Because the agent is still learning, if the agent exclusively took “greedy” exploitative actions, it may miss out on better policies that it hasn't discovered yet. On the other hand, if the agent exclusively explored with random actions, it would be completely discarding useful knowledge, and the time to find the optimal solution would be very slow. A common strategy to balance exploration and exploitation is an epsilon-greedy method, where the agent explores a random action with probability $\\small{\\epsilon}$ and chooses the action with the highest expected cumulative reward, $\\small{Q(W,t,a)}$, with a probability $\\small{1-\\epsilon}$. The simple epsilon-greedy strategy is implemented in Listing 12.5, although there are many more sophisticated approaches. For example, an agent could start off with a higher exploration rate and gradually reduce that rate as it learns more. Or, rather than exploring by indiscriminately choosing a random action, an agent can draw an action based on a probability distribution derived from the $\\small{Q}$ values for each action, exploring actions that are more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb198133",
   "metadata": {},
   "source": [
    "**Listing 12.5 Updating Q**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d82439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_Q(w_idx, t, Q):\n",
    "    if np.random.uniform() < epsilon: #A\n",
    "        a = np.random.randint(nA)\n",
    "    else:\n",
    "        A_array = Q[w_idx, t, :]\n",
    "        a = np.where(A_array == A_array.max())[0]\n",
    "        if (len(a)>1): #B\n",
    "            a = np.random.choice(a)\n",
    "    W_tplus1_idx = transition(W[w_idx,t], t, a)\n",
    "    Q[w_idx, t, a] += alpha*(gamma*Q[W_tplus1_idx, t+1, :].max() - Q[W_idx, t, a]) #C\n",
    "    return W_tplus1_idx, t+1, Q\n",
    "\n",
    "#A Explore with a probability epsilon\n",
    "#B If more than one action has the same Q, choose one of them randomly\n",
    "#C This implements the Q-Learning update equation that was discussed in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b202238",
   "metadata": {},
   "source": [
    "The final step in reinforcement learning is to generate thousands of episodes, or paths, to learn the function $\\small{Q}$, and consequently the optimal actions at each state. It has been proven mathematically that if the number of episodes approaches infinity, Q-learning will converge to the optimal policy. Listing 12.6 gives the Python code for generating paths. We initialized $\\small{Q}$ the same way we initialized $\\small{V}$ in the dynamic programming section by assigning a +1 or 0 depending on whether the final wealth at time $\\small{t=T}$ is greater than the goal, although for Q-learning this is unnecessary - we could have initiated the terminal values of $\\small{Q}$ to zero and the algorithm would have learned the terminal values. Also, notice that we added a progress bar using the `tqdm` library so that users can monitor the progress of the reinforcement learning algorithm and get an estimate of how long it will take to complete. Once the algorithm in Listing 12.6 is finished running on thousands of episodes, it is easy to see that the optimal policies and values from reinforcement learning should come close to those from dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6e9e3",
   "metadata": {},
   "source": [
    "**Listing 12.6 Generating Episodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba4976c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running...: 100%|██████████████████████████████████████████████████████████| 1000000/1000000 [06:06<00:00, 2726.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_episodes = 1000000\n",
    "epsilon = 0.5\n",
    "alpha = 0.1\n",
    "gamma = 1.0\n",
    "\n",
    "Q = np.zeros((nW, T+1, nA))\n",
    "for j in range(nW):\n",
    "    if W[j, T]>G:\n",
    "        Q[j, T, :] = 1\n",
    "\n",
    "for i in tqdm(range(n_episodes), desc=\"Running...\"):\n",
    "    W_idx = 0\n",
    "    t = 0\n",
    "    while t<=(T-1):\n",
    "        W_idx, t, Q = Update_Q(W_idx, t, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49822d",
   "metadata": {},
   "source": [
    "We mentioned several shortcomings in the goals-based investing example. For example, because of the binary reward of success or failure, all successes and failures count the same and the magnitude of success or failure is irrelevant. As a result, in low-wealth states, investors become extremely risk loving and will take huge risks in a desperate attempt to achieve their goal, and in high-wealth states, investors are indifferent between having \\\\$2 million and \\\\$20 million and become extremely risk-averse once their goal can be achieved with certainty. One could argue that this is a flaw, or oversimplification, of the binary objective function we used. And other objective functions have their own flaws. For example, if the objective is to maximize final period wealth, an investor will act as if he's risk neutral and allocate as much as possible to higher expected-return stocks, regardless of risk. A potential solution is to use utility functions in the objective function, which is the next topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe81e5",
   "metadata": {},
   "source": [
    "## 12.3 Utility Function Approach\n",
    "\n",
    "Economists have long used utility functions to quantify the enjoyment people derive from wealth or consumption. With a few exceptions, financial planners have not adopted utility functions in financial planning and decision-making. In this section, we will show how this area of economics can be applied to reinforcement learning, but first, we will explain what utility functions are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327da30",
   "metadata": {},
   "source": [
    "### 12.3.1 Understanding Utility Functions\n",
    "\n",
    "Suppose we wanted to define a utility function that represents how much pleasure an individual derives from different levels of wealth. Let's first think about what shape this function would take. We know that people prefer more wealth to less wealth, so the function would be upward-sloping. It is also reasonable to assume that the more wealth you have, the less happiness you'll derive from an additional dollar of wealth. In other words, if you're wealth is \\\\$1 and you find \\\\$10 under your couch, you will be happier than if you have \\\\$1 million and find $10, just like if you are hungry, the pleasure you get from the first scoop of ice cream is greater than the pleasure you would get from an additional scoop after you've already had 10 scoops. This is the principle of diminishing marginal utility and leads to a utility function that has a concave shape. An example of a concave utility function is shown in Figure 12.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5d108",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"500\" height=\"300\" src=\"Fig12_5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7f9f0",
   "metadata": {},
   "source": [
    "**Figure 12.5. Upward sloping concave utility function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b93ebc",
   "metadata": {},
   "source": [
    "A concave utility function also implies the individual is risk averse. To see that, suppose an individual wins a prize, determined by a coin flip. If he guesses correctly, he gets \\\\$10 million, and if he guesses incorrectly, he gets zero. He would much prefer to have a certain \\\\$5 million rather than participate in the coin flip. In fact, you can compute the “certainty equivalent” value of this prize. For the utility function in Figure 12.6, the certainty equivalent amount is \\\\$2.5 million, which means he would be indifferent between a sure \\\\$2.5 million and the coin flip between zero and \\\\$10 million. The degree of risk aversion is related to the curvature of the utility function. An individual with a straight-line utility function would be risk neutral rather than risk-averse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24127eab",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"500\" height=\"300\" src=\"Fig12_6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1ced6",
   "metadata": {},
   "source": [
    "**Figure 12.6. Certainty Equivalent value of a coin flip**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae18fd4",
   "metadata": {},
   "source": [
    "One utility function that is commonly used by economists is called the power utility function, which is given by the equation\n",
    "\n",
    "$$U(W) = \\frac{W^{1-\\eta}}{1-\\eta} \\text{ for } \\eta\\ge 0, \\eta \\ne 1$$\n",
    "\n",
    "The constant $\\small{\\eta}$ is referred to as the coefficient of Constant Relative Risk Aversion (CRRA). When $\\small{\\eta=0}$, the utility function is linear, which corresponds to risk neutrality. When $\\small{\\eta \\to \\infty}$, the utility function represents extreme risk aversion. And when $\\small{\\eta = }$, the utility function is defined as $\\small{U(W) = ln(W)}$. One of the challenges of using utility functions is estimating $\\small{\\eta}$, which varies by individual, and the challenge is not dissimilar to the challenge of mapping risk tolerance surveys to an asset allocation that we discussed in Chapter 2. There have been dozens of empirical studies in the Economics literature that have tried to estimate $\\small{\\eta}$, using lab experiments, surveys, auction behavior, labor supply behavior, insurance choices, and options prices. There are even several papers that try to estimate the parameters from the behavior on game shows like “Who Wants to Be a Millionaire” and “Deal or No Deal”. The range of estimates varies, and in the examples below, we will use a coefficient of $\\small{\\eta = 3}$.\n",
    "\n",
    "Now that we’ve explained utility functions, let’s see how they can be used in financial planning and decision-making. A simple extension of the goals-based investing example is to replace the binary reward of +1 or 0 with the utility of final wealth, $\\small{U(W_T)}$. This involves changing a single line of code: in the dynamic programming solution, the line of code that initializes the terminal value function, and in the Q-learning solution, the line of code that initializes $\\small{Q}$ with the final reward. But in the next section, we will consider a more complex extension, involving optimal spending."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d6bac",
   "metadata": {},
   "source": [
    "### 12.3.2 Optimal Spending Using Utility Functions\n",
    "\n",
    "In the goals-based investing example, the only action an agent takes at each state is choosing the percentage of stock holdings. Spending, an important consideration for financial planning, was completely ignored. We touched upon spending in Chapter 5 on Monte Carlo simulations. But in Chapter 5, when we tried to estimate how long an individual's money would last, we either assumed spending was fixed each year, or we adjusted it in a very simple way through guardrails, with no attempt at optimization. In this section, we will try to optimize how to choose spending each year as a function of an individual's wealth-time state to maximize expected utility, simultaneously optimizing asset allocation as well. \n",
    "\n",
    "This is a classic problem in financial economics, which you can find in a Google search for “optimal consumption and portfolio choice”. We will discuss later the difference between what economists call “consumption” and what financial planners and everyone else call “spending”, but for now, assume they are the same. Under some assumptions, there is a mathematical solution and numerical solutions are unnecessary. However, when we add additional assumptions later, it can only be solved numerically. The mathematical description of the problem is\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\large{a_t, \\ c_t}}{\\text{max}} & & E \\left[ \\sum_{t=0}^T \\gamma^t U(c_t) \\right] \\\\\n",
    "& \\text{subject to} & & W_{t+1} = (W_t - c_t) \\ (a_t {\\tilde{r}_t} + (1-a_t) r_f)\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where $\\small{a_t}$ is the allocation to stocks, $\\small{c_t}$ is consumption, $\\small{W_t}$ is wealth, $\\small{\\tilde{r}_t}$ is the random return on the stock portfolio, $\\small{U(c_t)}$ is the utility of consumption, all at time $\\small{t}$, and $\\small{\\gamma}$ is the subjective discount factor. The reward for each period is the utility of consumption, and we want to maximize the cumulative sum of discounted rewards. But we need a budget constraint, which shows how wealth evolves over time and places a constraint on spending. The next period's wealth is equal to this period's wealth, net of consumption this period, plus investment income. Labor income could also be included in the budget constraint, which we'll address later in the chapter.\n",
    "\n",
    "The code for solving this problem is very similar to the code for the goals-based investing problem. We will provide the code below and highlight the lines of code that change when we introduce utility functions and spending. As before, we start with creating a state space grid, which is shown in Listing 12.7. With spending, the state space grid will be slightly different from the original Listing 12.1 - the lower bound must be lower to account for lower wealth states after consumption depletes wealth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea216e",
   "metadata": {},
   "source": [
    "**Listing 12.7 Creating State Space for the Spending Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9ef6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = 1000000\n",
    "T = 10\n",
    "nW = 101\n",
    "mu_stock = 0.08\n",
    "mu_bond = 0.045\n",
    "sig_stock = 0.20\n",
    "sig_bond = 0.0\n",
    "\n",
    "Cbar = np.zeros(T+1)\n",
    "Cbar[0] = 0\n",
    "for t in range(1, T+1):\n",
    "    Cbar[t] = Cbar[t-1] + 1/(T-t+1)\n",
    "\n",
    "lnW = np.zeros((nW,T+1))\n",
    "W = np.zeros((nW, T+1))\n",
    "\n",
    "for t in range(T+1):\n",
    "    lnW_min = log(W0) - t*log(3) #A\n",
    "    lnW_max = log(W0) - Cbar[t] + (mu_stock-0.5*sig_stock**2)*t + 2.5*sig_stock*sqrt(t)\n",
    "    lnW[:,t] = np.linspace(lnW_min, lnW_max, nW)\n",
    "W = np.exp(lnW)\n",
    "\n",
    "#A The potential minimum wealth is lower because of consumption "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d027e4e",
   "metadata": {},
   "source": [
    "The dynamic programming code in Listing 12.8 has a few key differences from the corresponding code in Listing 12.3. Now, we cannot ignore the subjective discount rate, gamma, since the agent receives rewards every period from consumption. In the last period, all wealth is consumed so we set the value function at time $\\small{T}$ equal to the utility of final wealth, $\\small{U(W_T)}$. We not only have to loop through every wealth-time state and every discrete asset allocation, but now we also loop through every discrete level of consumption. In our example, consumption is a percentage of wealth ranging from 1.66\\%, 3.33\\%, … , 66.67\\%. When computing the transition probabilities emanating from wealth at time $\\small{t}$, we must subtract the consumption at time $\\small{t}$ now. And the calculation of the expected value at a node has an extra term: the (discounted) value next period, as before, but the current reward, which is the utility of consumption this period. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4244ae1",
   "metadata": {},
   "source": [
    "**Listing 12.8 Dynamic Programming Solution for  Spending Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1db2278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nA = 11\n",
    "nC = 40\n",
    "V = np.zeros((nW, T+1))\n",
    "A = np.zeros((nW, T))\n",
    "C = np.zeros((nW, T)) #A\n",
    "EV = np.zeros((nA, nC))\n",
    "W_midpoints = compute_midpoints()\n",
    "\n",
    "gamma = 0.95 #B\n",
    "CRRA = 3 #C\n",
    "def utility(c): #D\n",
    "    if (CRRA == 1):\n",
    "        util = log(c)\n",
    "    else:\n",
    "        util = (c**(1-CRRA))/(1-CRRA)\n",
    "    return util\n",
    "\n",
    "for j in range(nW):\n",
    "        V[j, T] = utility(W[j,T]) #E \n",
    "        \n",
    "for t in range(T-1, -1, -1):\n",
    "    for j in range(nW):\n",
    "        for a in range(nA):\n",
    "            for c in range(nC):\n",
    "                cons = (c+1)/(1.5*nC)*W[j,t] #F\n",
    "                W_tplus1_dist = compute_transition_probs(W[j,t]-cons, t, a) #G\n",
    "                EV[a,c] = np.dot(W_tplus1_dist, utility(cons)+gamma*V[:,t+1]) #H\n",
    "        V[j,t] = EV.max()\n",
    "        A[j,t], C[j,t] = np.unravel_index(EV.argmax(), EV.shape) #I\n",
    "\n",
    "#A Initialize the two-dimensional optimal consumption\n",
    "#B The subjective discount rate\n",
    "#C The coefficient of constant relative risk aversion\n",
    "#D Power utility function\n",
    "#E In the last period, all wealth is consumed\n",
    "#F Consumption is a percentage of wealth\n",
    "#G Consumption is subtracted from wealth before computing the transition probabilities\n",
    "#H Current value is utility from consumption plus discounted expected value next period\n",
    "#I Returns the row (asset allocation) and column (consumption) for optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990373c0",
   "metadata": {},
   "source": [
    "For the reinforcement learning solution with spending, the code in Listing 12.4 that computes a random transition from wealth at time $\\small{t}$ to a new wealth at time $\\small{t+1}$ does not change, since we already accounted for consumption in the starting wealth. In Listing 12.9, the function that implements an epsilon-greedy strategy for updating $\\small{Q}$ is very similar to the code in Listing 12.5. The code that updates $\\small{Q}$ now includes the current reward, which is the utility of current consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f450ff8",
   "metadata": {},
   "source": [
    "**Listing 12.9 Updating Q for Spending Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0d7f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_Q(w_idx, t, Q):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        a = np.random.randint(nA)\n",
    "        c = np.random.randint(nC)\n",
    "    else:\n",
    "        AC_array = Q[w_idx,t,:,:]\n",
    "        ac = np.argwhere(AC_array == AC_array.max()) #A\n",
    "        if (len(ac)>1):\n",
    "            a,c = ac[np.random.choice(len(ac))]\n",
    "        else:\n",
    "            a,c = ac[0]\n",
    "    cons = (c+1)/(1.5*nC)*W[w_idx,t]\n",
    "    W_tplus1_idx = transition(W[w_idx,t]-cons, t, a) #B\n",
    "    Q[w_idx,t,a,c] += alpha*(utility(cons) + gamma*Q[W_tplus1_idx,t+1,:,:].max() -\n",
    "                                                   Q[W_idx,t,a,c]) #C\n",
    "    return W_tplus1_idx, t+1, Q\n",
    "\n",
    "#A Row (asset allocation) and column (consumption) of highest Q\n",
    "#B Consumption is subtracted from wealth before computing random next state\n",
    "#C This implements the Q-Learning update equation that was discussed in the text, including current consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e348a16",
   "metadata": {},
   "source": [
    "Finally, generating episodes in Listing 12.10 is identical to the code in Listing 12.6 except that $\\small{Q}$ is initialized at $\\small{t=T}$ to the utility of consuming all the wealth, and we also increased the number of episodes to reflect the fact that we have a larger space of possible actions to choose with both consumption and asset allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b6a71",
   "metadata": {},
   "source": [
    "**Listing 12.10 Generating Episodes for Spending Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89ca7b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running...: 100%|██████████████████████████████████████████████████████████| 5000000/5000000 [35:35<00:00, 2341.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_episodes = 5000000\n",
    "epsilon = 0.5\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "\n",
    "Q = np.zeros((nW, T+1, nA, nC))\n",
    "for j in range(nW):\n",
    "        Q[j,T,:,:] = utility(W[j,T]) #A\n",
    "\n",
    "for i in tqdm(range(n_episodes), desc=\"Running...\"):\n",
    "    W_idx = 0\n",
    "    t = 0\n",
    "    while t<=(T-1):\n",
    "        W_idx, t, Q = Update_Q(W_idx, t, Q)\n",
    "\n",
    "#A Initialize Q in the final time period to the utility of final wealth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770272af",
   "metadata": {},
   "source": [
    "As you might guess from the objective function, spending is optimized when consumption is smoothed over time. It would not be wise to spend a large amount of wealth early on, giving a high reward now but a low reward in the future. For example, at the second to last time period, $\\small{t=T-1}$, you would want to spend approximately half your wealth, saving the other half for the last time period. Also, if you adjust the parameters of the model, you can see the effects on spending:\n",
    "\n",
    "* Higher expected returns lead to more spending today because you have higher expected wealth next period for consumption smoothing (for those interested in that relationship, you can explore the Euler Equation on `github.com/robreider/robo-advisor-with-python/Chapter-12.ipynb`).\n",
    "* Higher degrees of risk aversion leads to more savings and less spending today. \n",
    "* Higher subjective discount rates, which measure the preference for immediate consumption, of course, lead to more spending today.\n",
    "\n",
    "One of the largest shortcomings of the model we described in this section is that the agent knows he has exactly $\\small{T}$ years to live. For example, because of that highly unrealistic assumption, the agent can safely spend approximately half his wealth at $\\small{t=T-1}$ knowing that he will have the other half for the last year. In the next section, we will extend the model to include mortality tables which will allow us to solve a richer set of financial planning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe4639",
   "metadata": {},
   "source": [
    "## 12.4 Longevity Risk\n",
    "\n",
    "The uncertainty in life expectancy has an enormous impact on financial planning. Individuals not only face stock market risk but also the risk that they outlive their assets, which is referred to as longevity risk. Despite the importance of longevity risk, most financial planning tools are not equipped to handle it. We mentioned in Chapter 1 that a vast majority of Social Security calculators compute the present value of Social Security payments, which is essentially a risk-neutral approach. These calculators suggest claiming Social Security later if the life expectancy of an individual exceeds the break-even age. \n",
    "\n",
    "Using lifecycle models with utility functions is a significant improvement over these other techniques for handling longevity risk. And only a few lines of code need to be changed to incorporate an uncertain date of death. We won’t repeat the code again, but it can be found in `github.com/robreider/robo-advisor-with-python/Chapter-12.ipynb`.\n",
    "\n",
    "Consider a 62-year-old individual who is deciding whether to claim Social Security right away at 62 vs waiting until 70 to take a 76\\% higher payout. Using the mortality tables from Chapter 5, the life expectancy for a 62-year-old is about 84 years old, which is a little above the break-even age, suggesting that it's better to delay claiming Social Security. But it's a close call, and the present value of claiming at 70 is only about \\\\$3,000 more than claiming at 62. However, when we take into account longevity risk, it's not such a close call. The utility for claiming at 70 is significantly higher than the utility for claiming at 62. But how can we translate the difference in utility to something meaningful, like dollars? We can use the concept of certainty equivalence that we discussed earlier in the chapter. For the individual who claims at 62, we can incrementally increase his starting wealth from \\\\$1mm and stop when his utility equals the utility of the person who claims at 70. That number is \\\\$35,000 for someone with a coefficient of relative risk aversion of 3. To put that number in perspective, the \\\\$35,000 extra for claiming later is more than a year's worth of Social Security payments. In Figure 12.7, we show the higher values for claiming Social Security at 70 for three levels of risk aversion. As you would expect, the higher the risk aversion, the greater the value of waiting to claim Social Security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdef9f0",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"500\" height=\"300\" src=\"Fig12_7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8669c",
   "metadata": {},
   "source": [
    "**Figure 12.7. Quantifying the benefit of claiming Social Security at 70 vs 62**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b10367",
   "metadata": {},
   "source": [
    "We made several assumptions in this analysis, and the differential between claiming Social Security at 70 vs 62 will vary, depending on the assumptions. Here are a few of these critical assumptions:\n",
    "* We used the mortality tables from the Society of Actuaries that were described in Chapter 5. Of course, if life expectancies are shorter than the average life expectancy, then the benefit of delaying is reduced.\n",
    "* We assumed a real rate of interest (the difference between the nominal rate of interest and inflation) of 2\\% (4.5\\% bond return and 2.5\\% inflation rate). Since Social Security payments grow with inflation, inflation rate assumptions by themselves would not necessarily affect the claiming decision, but the difference between nominal rates and inflation would. For example, if we assumed a real rate lower than 2\\%, then the benefit of delaying is even greater.\n",
    "* We only considered the optimization from the perspective of an individual, not a married couple. For example, if your spouse had a short earnings history, they can receive spousal and survivor benefits based on your work history. Although delaying claiming will not increase the spousal benefit, it will increase the survivor benefit if you die before your spouse, which further increases the benefit of delaying.\n",
    "\n",
    "The same type of analysis we employed here can be used to answer other financial planning questions. For example, many employees with defined benefit pension plans must choose between taking their payout as a lump sum or a steady stream of lifetime income. We can use the same lifecycle model to analyze this decision, with some small modifications (for example, pension payments typically don’t have inflation adjustments). Another similar financial planning decision individuals face is whether to purchase annuities from an insurance company. In fact, because of provisions in the Secure Act that was passed in 2019, many more 401(k) plans are offering annuities as an option. These annuities reduce longevity risk but often come with high fees. The type of annuities offered may also vary, ranging from annuities that start paying income immediately to deferred annuities that delay income payments. The lifecycle models we described can be used to analyze the various options and can be customized to an individual’s circumstances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f00fbb",
   "metadata": {},
   "source": [
    "## 12.5 Other Extensions\n",
    "\n",
    "There are numerous ways to make the model even more realistic, and even though we will leave it up to the reader to explore, we will give a few examples to conclude this chapter. Here is a list of a few things to consider for extending the model, but this is not meant to be an exhaustive list:\n",
    "\n",
    "* We did not include labor income during the accumulation phase of an individual’s life cycle. Empirical studies show that spending is highly correlated to labor income. And uncertainty in future labor income leads to more “precautionary savings”. Similarly, we did not include medical expenses, which are also uncertain and may be treated separately from other types of consumption: usually medical expenses are treated as necessary costs and no utility is derived from them.\n",
    "* Although taxes have been discussed in other chapters, we have not incorporated them into the reinforcement learning model. For example, we have simply assumed that money for spending comes from wealth, without considering the tax implications and the optimal sequence of withdrawals, which is discussed in Chapter 14.\n",
    "* We have assumed spending and consumption are the same thing. But economists distinguish spending on non-durable goods, which are immediately consumed, from spending on durable goods. Consider, for example, a house as a durable good.  Economists argue that only a small percentage of the value of a house is “consumed”, in the form of housing services, which would be the imputed rent on an owner-occupied home, or in other words, the rent they would pay on an equivalent house. Also, an individual can’t increase their housing services by 1% if they get a little wealthier, because upgrading from a smaller house to a larger house involves significant transaction costs and can’t be incrementally modified like a stock or bond holding.\n",
    "* There is extensive literature on alternatives to the utility function that we used. For example, prospect theory, developed by behavioral economists Kahneman and Tversky, posits that losses have a greater emotional impact than equivalent gains, leading to an asymmetric utility function around some reference point, like current wealth or spending.\n",
    "* We have ignored the “bequest motive” and assumed the agent receives no reward for any money he passes down to his heirs. It is straightforward to capture the bequest motive by adding an extra reward upon death that is a function of the wealth that is passed down. And economists have tried to estimate a constant representing the utility one derives from passing a dollar to heirs compared to the utility from consuming that dollar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f56b5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this chapter we have learned:\n",
    "* Reinforcement learning, a branch of AI, has been successfully applied to many tasks and can be applied to multiperiod financial planning problems as well \n",
    "* Goals-based investing involves strategies for achieving a savings goal, such as saving a certain amount for retirement or for buying a home\n",
    "* The traditional approach to finding the optimal asset allocation over multiple periods is to use dynamic programming, working backwards from the terminal state\n",
    "* The advantage of reinforcement learning over dynamic programming is that you don’t need to know the probability distribution of transitioning to the next state, and dynamic programming suffers from the curse of dimensionality when there are too many states and potential actions \n",
    "* The glide path, which is the optimal allocation of stocks over time, can take many shapes depending on the wealth path\n",
    "* Using utility functions has not been widely adopted by financial planners but is a useful tool for handling financial planning decisions involving risk\n",
    "* Spending and asset allocation can simultaneously be optimized using reinforcement learning\n",
    "* The risk of outliving one’s assets because of uncertainty in life expectancy is referred to as longevity risk and is generally not properly captured with existing financial planning tools\n",
    "* Reinforcement learning that incorporates mortality tables can be used to answer financial planning questions like when to claim Social Security, whether to choose a lump sum pension payout, and whether to buy an annuity\n",
    "* When longevity risk is properly accounted for, the benefit of receiving payments for life is much larger than a simple break-even analysis would suggest\n",
    "* The models can easily be extended in many directions, by including labor income and health expenses, taxes, housing decisions, a bequest motive, and alternate utility functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
